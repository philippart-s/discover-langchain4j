### Global configurations
# Base URL for Mistral AI endpoints
quarkus.langchain4j.ollama.base-url=http://localhost:11434/
# Activate or not the log during the request
quarkus.langchain4j.ollama.log-requests=true
# Delay before raising a timeout exception                    
quarkus.langchain4j.ollama.timeout=60s    
# Activate or not the Mistral AI embedding model                      
quarkus.langchain4j.ollama.embedding-model.enabled=false

### Chat model configurations
# Activate or not the Mistral AI chat model
quarkus.langchain4j.ollama.chat-model.enabled=true              
# Chat model name used
quarkus.langchain4j.ollama.chat-model.model-id=mistral
